{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "779498ee-8a7d-46ce-ac1d-a02aa937b07e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from libs import schema_handler as sh\n",
    "from libs import datafunctions as spdf\n",
    "from libs import utils as ut\n",
    "from libs import azureauth as az\n",
    "from delta.tables import *\n",
    "from datetime import date, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f574686c-6a9e-4ccb-80f8-f6c42ee19da2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_table = dbutils.widgets.get('silver_table')\n",
    "gold_table = dbutils.widgets.get('gold_table')\n",
    "source = dbutils.widgets.get('source')\n",
    "watermark = dbutils.widgets.get('watermark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08b0364f-78a9-4b80-9490-6faf99058134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "env = az.get_env(spark)\n",
    "#watermark_timestamp = \"DH_\"+silver_table.split('_')[1].upper()+\"_\"+ut.format_timestamp(watermark)\n",
    "field_schema, primary_keys, table_type, dependencies = sh.read_schema(env = env, layer= \"gold\", source= source, entity=gold_table)\n",
    "surrogate_keys = sh.extract_surrogate_keys(field_schema)\n",
    "merge_condition = spdf.generate_merge_condition(surrogate_keys=surrogate_keys)\n",
    "print(table_type)\n",
    "print(merge_condition)\n",
    "print(field_schema)\n",
    "print(surrogate_keys)\n",
    "print(dependencies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7245dfec-381e-4dcd-9132-9ce7a0270d40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#to remove the unnecessary surrogate keys. can also implement a function. but not in mood right now.\n",
    "\n",
    "\n",
    "dep_sk = [dep[\"surrogateKey\"] for dep in dependencies]\n",
    "\n",
    "surrogate_keys = [sk for sk in surrogate_keys if sk not in dep_sk]\n",
    "print(surrogate_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a943ae23-558a-4c86-8111-f9cdcb961a5e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756737252824}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfSilver = spark.read.table(f\"datahub_{env}_silver.{source}.{silver_table}\").filter(F.col(\"dbx_extract_timestamp\") > watermark)\n",
    "display(dfSilver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d153f97d-b9a2-4775-b9f6-fa9f977f9ab4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756741425304}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#in case of dependencies we need to resolve them. like we need to link to their surrorate keys first.\n",
    "if dependencies:\n",
    "    if not spdf.check_dependencies_exist(spark, dependencies):\n",
    "        raise Exception(\"Dependency tables not found\")\n",
    "    \n",
    "    dfSilver = spdf.resolve_dependencies(spark,dfSilver, dependencies)\n",
    "\n",
    "display(dfSilver)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc5fccfc-e9f5-4860-ace4-3fcfa97fee41",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756578427902}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#In case we have dim tables then we need to add the surrogate key\n",
    "if table_type ==\"SCD1\":\n",
    "    if not sh.uc_table_exists(spark, catalog= f\"datahub_{env}_gold\", schema = source, table = gold_table):\n",
    "        dfSilver = spdf.assign_surrogate_key(spark, dfSilver, f\"datahub_{env}_gold\", source, silver_table, surrogate_keys[0])\n",
    "        display(dfSilver)\n",
    "    else:\n",
    "        #Table exists then again we need to divide the table in 2 halves.\n",
    "        dfGold = spark.read.table(f\"datahub_{env}_gold.{source}.{gold_table}\")\n",
    "        dfCombined = dfSilver.join(dfGold, spdf.generate_join_condition(dfSilver, dfGold, primary_keys), how = \"left\")\n",
    "        #display(dfCombined)\n",
    "\n",
    "        dfSilver_old = dfCombined.filter(F.col(surrogate_keys[0]).isNotNull())\\\n",
    "                        .select(dfSilver[\"*\"], dfGold[surrogate_keys[0]], dfGold[\"create_date\"])\n",
    "        dfSilver_new = dfCombined.filter(F.col(surrogate_keys[0]).isNull())\\\n",
    "                        .select(dfSilver[\"*\"])\n",
    "        #assign new surrogate keys to the new entries\n",
    "        dfSilver_new = spdf.assign_surrogate_key(spark, dfSilver_new, f\"datahub_{env}_gold\", source, silver_table, surrogate_keys[0])\n",
    "        dfSilver_old = dfSilver_old.withColumn(\"update_date\", F.current_date())\n",
    "        dfSilver_new = dfSilver_new.withColumn(\"create_date\", F.current_date()).withColumn(\"update_date\", F.current_date())\n",
    "\n",
    "        dfSilver = dfSilver_old.union(dfSilver_new)\n",
    "        display(dfSilver)\n",
    "\n",
    "\n",
    "elif table_type ==\"SCD2\":\n",
    "    #In case gold table doesn't exist which means it is the first time run in this case just assign the surrogate keys\n",
    "    if not sh.uc_table_exists(spark, catalog= f\"datahub_{env}_gold\", schema = source, table = gold_table):\n",
    "        dfSilver = spdf.assign_surrogate_key(spark, dfSilver, f\"datahub_{env}_gold\", source, silver_table, surrogate_keys[0])\n",
    "        display(dfSilver)\n",
    "    else:\n",
    "        #table exists then now we need to find existing and the new records. For this apply the left join. And then we will be splitting the table in two halves.\n",
    "        #This thing we are doing in order to assign new surrogate keys to the new records. and old records will be getting old surrogate keys only.\n",
    "\n",
    "\n",
    "        #here also the challenge is that we need to update only create only those old records which are having different values as compared to the gold table. \n",
    "\n",
    "        #also we need active records only.\n",
    "\n",
    "\n",
    "        dfGold = spark.read.table(f\"datahub_{env}_gold.{source}.{gold_table}\").filter(F.col(\"effective_end_date\") == '9999-12-31' ) \n",
    "        dfCombined = dfSilver.join(dfGold, spdf.generate_join_condition(dfSilver, dfGold, primary_keys), how = \"left\")\n",
    "        \n",
    "        #display(dfCombined)\n",
    "\n",
    "        #display(dfGold)\n",
    "\n",
    "        dfSilver_old = dfCombined.filter(F.col(surrogate_keys[0]).isNotNull())\\\n",
    "                        .select(dfSilver[\"*\"], dfGold[surrogate_keys[0]])\n",
    "\n",
    "\n",
    "\n",
    "        #now we need to find what are the records what we need to update from the old records. \n",
    "        dfSilver_old_modified = sh.get_changed_new_records(dfGold, dfSilver_old, surrogate_keys[0], [\"extract_timestamp\", \"dbx_extract_timestamp\", \"effective_start_date\", \"effective_end_date\"])\n",
    "\n",
    "\n",
    "        #need to reorder it for the union as sk was coming in front\n",
    "\n",
    "        dfSilver_old_modified = sh.reorder_with_sk_last(dfSilver_old_modified, surrogate_keys)\n",
    "\n",
    "\n",
    "        #now the thig is we need to mark only these records as inactive in the scd2\n",
    "        #we will make union and add them to the silver then we will assign new surrogate keys to these old records and add to the union.\n",
    "\n",
    "\n",
    "\n",
    "        dfSilver_new = dfCombined.filter(F.col(surrogate_keys[0]).isNull())\\\n",
    "                        .select(dfSilver[\"*\"])\n",
    "\n",
    "        #display(dfSilver_old)\n",
    "        dfSilver_new = spdf.assign_surrogate_key(spark, dfSilver_new, f\"datahub_{env}_gold\", source, silver_table, surrogate_keys[0])\n",
    "        #display(dfSilver_new)\n",
    "\n",
    "        dfSilver = dfSilver_old_modified.union(dfSilver_new)\n",
    "        #display(dfSilver)\n",
    "\n",
    "\n",
    "        #after the union we need to drop the surrogate keys and assign the new ones but here we need to use dfSilver_new or else sk will be clashed\n",
    "        if dfSilver_new.count() >0:\n",
    "            max_sk_value = dfSilver_new.agg(F.max(surrogate_keys[0]).alias(\"max_sk_val\")).collect()[0][\"max_sk_val\"]\n",
    "            print(max_sk_value)\n",
    "            dfSilver_old_modified = dfSilver_old_modified.drop(*surrogate_keys)\n",
    "            dfSilver_old_modified = dfSilver_old_modified.withColumn(surrogate_keys[0], F.lit(max_sk_value+1) + F.monotonically_increasing_id())\n",
    "        #display(dfSilver_old_modified)\n",
    "\n",
    "\n",
    "        #now we need to prepare final dfSilver after the union. so in this way we will be able to handle both of the things with our scd_2 merge\n",
    "        dfSilver = dfSilver.union(dfSilver_old_modified)\n",
    "        display(dfSilver)\n",
    "elif table_type ==\"FACT\":\n",
    "    dfSilver = dfSilver.withColumn(\"create_date\", F.current_date()).withColumn(\"update_date\", F.current_date())\n",
    "    display(dfSilver)\n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2003d072-a612-48f1-a065-2fe86097eb7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#delta_table = DeltaTable.forName(spark, f\"datahub_{env}_silver.{source}.{silver_table}\")\n",
    "\n",
    "print(datetime.strptime('9999-12-30', '%Y-%m-%d').date() - date.today())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42396f1d-a392-482c-8be0-70a48fc60c48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_path = f\"abfss://gold@storageawesum.dfs.core.windows.net/data/{source}/{gold_table}\"\n",
    "\n",
    "\n",
    "if not sh.uc_table_exists(spark, catalog= f\"datahub_{env}_gold\", schema = source, table = gold_table):\n",
    "\n",
    "#this is for the first time run. In this case there is not table and hence are not focussed on schema evolution\n",
    "\n",
    "    if table_type == \"SCD1\":\n",
    "        create_date = date.today()\n",
    "        update_date = date.today()\n",
    "        dfSilver = dfSilver.withColumn(\"create_date\", F.lit(create_date))\n",
    "        dfSilver = dfSilver.withColumn(\"update_date\", F.lit(update_date))\n",
    "        display(dfSilver)\n",
    "        \n",
    "    elif table_type ==\"SCD2\":\n",
    "        effective_start_date = date.today()\n",
    "        effective_end_date = '9999-12-31'\n",
    "        dfSilver = dfSilver.withColumn(\"effective_start_date\", F.lit(effective_start_date))\n",
    "        dfSilver = dfSilver.withColumn(\"effective_end_date\", F.to_date(F.lit(effective_end_date)))\n",
    "        display(dfSilver)\n",
    "\n",
    "\n",
    "    \n",
    "    dfSilver.write.mode(\"overwrite\").format(\"delta\").partitionBy('extract_timestamp').save(gold_path)\n",
    "\n",
    "\n",
    "    \n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE SCHEMA IF NOT EXISTS datahub_{env}_gold.{source}\n",
    "    \"\"\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS datahub_{env}_gold.{source}.{gold_table}\n",
    "        USING delta\n",
    "        LOCATION '{gold_path}'\n",
    "    \"\"\")\n",
    " \n",
    "\n",
    "else:\n",
    "    #handle schema evolution in another runs first\n",
    "    schema_evolved = sh.handle_schema_evolution(\n",
    "        spark = spark,\n",
    "        source_df = dfSilver.limit(0),\n",
    "        target_path=gold_path,\n",
    "        contract_attributes = sh.extract_attributes(field_schema),\n",
    "        mode = \"append\"\n",
    "    )\n",
    "    # in case we get green signal like schema can be evolved then we will handle the merge logic\n",
    "    if schema_evolved:\n",
    "        if table_type == \"SCD1\":\n",
    "            merge_happened = spdf.scd1_merge(\n",
    "                spark,\n",
    "                dfSilver,\n",
    "                f\"datahub_{env}_gold.{source}.{gold_table}\",\n",
    "                surrogate_keys\n",
    "            )\n",
    "            if merge_happened:\n",
    "                print(\"SCD1 Performed\")\n",
    "            else:\n",
    "                print(\"SCD1 Not Performed\")\n",
    "\n",
    "        elif table_type ==\"SCD2\":\n",
    "            merge_happened = spdf.scd2_merge(\n",
    "                spark,\n",
    "                dfSilver,\n",
    "                f\"datahub_{env}_gold.{source}.{gold_table}\",\n",
    "                surrogate_keys\n",
    "            )\n",
    "            if merge_happened:\n",
    "                print(\"SCD2 Performed\")\n",
    "            else:\n",
    "                print(\"SCD2 Not Performed\")\n",
    "\n",
    "        elif table_type ==\"FACT\":\n",
    "            dfSilver.write.mode(\"append\").partitionBy('extract_timestamp').format(\"delta\").save(gold_path)\n",
    "            print(\"FACT Table Updated\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51188aec-3460-473a-99c0-072a50c7969d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "load_in_gold_generic",
   "widgets": {
    "gold_table": {
     "currentValue": "fact_orders",
     "nuid": "0d27c322-5b68-4222-8156-ffed53c90a3d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "gold_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "gold_table",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "silver_table": {
     "currentValue": "fact_orders",
     "nuid": "215b29c0-8b68-4bed-b43f-c7e7843e3695",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "silver_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "silver_table",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "source": {
     "currentValue": "consumer",
     "nuid": "8115df1c-5057-49ec-b676-e9b60926e61f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "source",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "source",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "watermark": {
     "currentValue": "1900-08-24T08:14:27.213+00:00",
     "nuid": "513a6a3a-5a32-4543-a902-3eeb88f299ae",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "watermark",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "watermark",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
